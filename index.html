<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>wEbola: A Predictive Model for Ebola by lclukechang</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">wEbola: A Predictive Model for Ebola</h1>
        <p class="header">A Harvard CS109 Final Project by Julie Chang, Luke Chang, Kevin Hu, Jiho Kang</p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/lclukechang/CS109-Project/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/lclukechang/CS109-Project/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/lclukechang/CS109-Project">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/lclukechang">lclukechang</a></p>


      </header>
      <section>
        <h3>
<a id="overview-and-motivation" class="anchor" href="#overview-and-motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview and Motivation</h3>

<p>Inspired by the recent influx of news coverage of Ebola in the United States, we decided to investigate and conduct further research about Ebola in West Africa. Prior to this class, we all had interests in global health issues, and we thought this would be an incredible topic to explore that not only corresponds with our passions, but also would be relevant long after our project’s conclusion. We noticed that the current fight against Ebola has been largely ineffective and disorganized, so we thought this may be due to a lack of understanding on how to best distribute the resources. We are hoping to parse through the data in a clear and methodical way that will help us better prepare for the spread of Ebola in the future.</p>

<p>Ultimately we want to be able to predict the spread of Ebola, and come up with a predictive model that takes in as inputs demographic information and regional data to predict. Through this project, we hope to provide a comprehensive and rigorous analysis of the data available on Ebola and use it to better inform health workers, governments, and NGOs, and provide actionable recommendations for future Ebola interventions based off of the analysis we conducted. The benefits from this project will be multifold. First, we will have a greater understanding of the epidemic and the factors which are most important in determining spread and also intervention. We will also provide a service to the Ebola community, and take part in the fight against Ebola, serving as another resource to better and more efficiently allocate time and money towards this epidemic.</p>

<h3>
<a id="related-work" class="anchor" href="#related-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Related Work</h3>

<p>We were inspired largely by the HackEbola Hackathon (<a href="http://hackebolawithdata.challengepost.com/">http://hackebolawithdata.challengepost.com/</a>), from which we received our data. Our project originally started as a hackathon project that we decided to complete through the context of this final project. We also drew analytical insights from work that we did in class, such as from Problem Set 4, where we classified characteristics using machine learning techniques.</p>

<h3>
<a id="initial-questions" class="anchor" href="#initial-questions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Initial Questions</h3>

<p>The main question we were trying to answer was where the epidemic would spread next, and how many cases could be expected. We originally began with the intention of determining which regional factors wouldb e most important in predicting Ebola spread. However, as we continued our anaylsis it beame clear that this would not be possible in the scope of our project through our use of Principal Component Analysis in our model.
We took into consideration the limitations of our datasets, especially concerning the lack of datapoints available in many of the regions, and made the decision to consolidate them into one large meta-region, disregarding travel restrictions as unnecessary due to the fact that an overwhelmingly large percentage of the cases were located in Liberia.
Additionally, we also needed to address the question of what kind of data was appropriate to use. For example, although we had fairly comprehensive demograhics data, there were a lot of missing values that may have confounded our results. The decision to exclude that variable, or to use what data was available had to be made on a case-by-case basis. This decision to exclude or include data was especially important when trying to resolve conflicting naming practices for our regions of interest. Although we had done our best to sanitize our data, sometimes it was unclear from the documentation what some variables were. Again, the decision to use or discard data was left to personal discretion.</p>

<h3>
<a id="data" class="anchor" href="#data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data</h3>

<p>The source of the data came from a database for a hackathon held at Harvard College a few weekends ago called HackEbola. The data was drawn from HumanitarianResponse.info, the Open Humanitarian Data Repository, and the Humanitarian Data Exchange and compiled into a single resource that was hosted on qdatum (link: <a href="http://www.qdatum.io/public-sources">http://www.qdatum.io/public-sources</a>). We scraped the data we used for our model from qdatum and decided to pick only the datasets that were relevant to us. We mainly pulled data from Liberia and its neighboring countries. We decided to remove certain countries under our discretion if we were unable to find data on that country that spanned the datasets were interested. This included data on demographics, food prices (as as proxy for standard of living), movement restrictions, and basic epidemiological data (cases per region per date). Thus, for countries like Cote d'Ivoire that had demographics data but did not have basic epidemiological data, we chose to remove it due to lack of data we could model. Due to the non-standardization of the food price data, we ultimately decided not to incorporate it into our final model.
Though some primary processing was performed by the hosts of the event, we still had a substantial amount of cleanup to do. First, we noticed that names of the regions we explored were not standardized. Sometimes a certain county would just be called by the name of the county itself, sometimes the names of the regions would have quotes around it, and sometimes there would be inconsistencies in naming that do not fall into these categories. We standardized the names of the regions by renaming region values and removing erroneous data that were also coincidentally poorly named.
Movement restriction data was extracted from the HackEbola datasets. To prepare the dataset as time-series regressors, date ranges for movement restrictions were binarily interpolated (e.g. An Airport Closure from September 19th to October 19th would be interpolated into 30 days of Airport Closures with a value of 1 if restricted and 0 otherwise). Potential closures were excluded. Finally, datasets were grouped by country and region. This dataset was ultimately excluded from final analysis due to decisions we made on the regionality of our model, namely that because the vast majority of cases were in one country, imposing travel restrictions would not have significantly affected our model.
We also cleaned up the dating processes. Especially in the dataset containing the basic epidemiological data, the dates were labeled as an arbitrary value of days after a certain unknown date. In order to standardize this, we decided to standardize the date column to represent the number of dates after the first observation of Ebola overall by finding the minimum value in the column and subtracting every date with that value, thus resulting in days after the first observation in all of the data. This helped standardize the dates to create a clean model in the end with an appropriate dating mechanic.</p>

<h3>
<a id="exploratory-analysis" class="anchor" href="#exploratory-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exploratory Analysis</h3>

<p>Our overall model for the spread of ebola is a generalized logistic function of the form $\hat{C}_r(t) = \frac{Y_r}{1 + \exp[-K_r (t - C_r)}$, where $C_r(t)$ is the number of cases in region $r$ at time $t$, and $Y_r, K_r, C_r$ are three constants associated with region $r$.</p>

<p>This logistic function produces a sigmoid curve (S shape), which at first glance is reasonable as a model for disease outbreak; at first cases grow quickly as disease hosts are readily available and few protective health measures are being implemented - cases then slow their growth as either protective health measures slow the spread of the disease or as the new case and death dynamics reach equilibrium. Note that $Y_r$ is the maximum value of the function; it represents the susceptibility of the general population to the disease in general. $K_r$ is a growth rate, governing how quickly the disease spreads initially. $C_r$ is a time-centering parameter, which can include concepts such as the disease incubation rate (for example, a disease with a long incubation rate can take time from introduction to the first noted case, after which the disease spreads more rapidly).</p>

<p>This model is frequently used to describe population growth with population limitations such as scarcity of resources or some other carrying capacity. In this case, the model is perhaps most intuitively viewed as a differential equation: $\frac{dC}{dt} = rC \left(1 - \frac{C}{P} \right)$. We have $r$ representing a growth rate, $C$ representing the number of cases, and $P$ representing the carrying capacity; as $C$ is low, $dC/dt$ is positive, so $C$ increases, but as $C \to P$, the growth rate decreases since $1 - \frac{C}{P} \to 0 \Rightarrow \frac{dC}{dt} \to 0$. The $rC$ term accounts for exponential population/case growth without population limitations, while the $1 - \frac{C}{P}$ term accounts for downward pressure of overpopulation.</p>

<p>Our model then assumes that the spread of ebola fits a logistic functional form for each region. If this is the case, and if we can accurately determine parameters for the logistic functions based on characteristics of the regions, then we can accurately fit the growth pattern of ebola in any region through one estimator.</p>

<p>We in fact use a linear regression to determine the logistic function’s parameters. We have estimators $\begin{pmatrix} \hat{Y}_r \ \hat{K}_r \ \hat{C}_r \end{pmatrix} = \begin{pmatrix} - &amp; v_Y &amp; - \ - &amp; v_K &amp; - \ - &amp; v_C &amp; - \end{pmatrix} \begin{pmatrix} | \ D_r \ | \end{pmatrix}$ where $v_Y, v_K, v_C$ are the coefficients of regression and $D_r$ is the demographic information for region $r$. As we minimize the sum of squared residuals (SSR), we achieve $\hat{Y}_r, \hat{K}_r, \hat{C}_r \to^D Y_r, K_r, C_r$ (where $\to^D$ is convergence in distribution), so by the Continuous Mapping Theorem (allowing us to exponentiate) and by Slutsky’s Theorem (allowing us to combine the estimators through summation or multiplication), we know that $\hat{C}_r(t) \to^D C_r(t)$; our estimator for the number of cases in each region at each time is robust.</p>

<p>One issue that we face is high multicorrelation within the independent variables (i.e. the demographic statistics). Many of the independent variables are intuitively linked, often relating to overall income and income distribution; for example, the per-capital wealth index is likely linked to the state of flushing toilets in a region, as the latter is a proxy for technological development and for public infrastructure in the region. Thus, in order to avoid spurious results in our analysis, we must reduce the dimensionality of our independent variables. We accomplish this through Principal Component Analysis, orthogonalizing our multicorrelated data by specifying the number of components we will be using in our regression. Ultimately this diminishes the intuition behind the variables we use in the regression, as the orthogonalization transformation has no inuitive meaning in the context of the variables; however, this is a necessary trade-off in order to avoid overfitting and in order to decorrelate variables and reduce dimensionality. In order to determine how many components to use in PCA, we examine the explained variance for each number of components over a small range, and set an appropriate cutoff for the minimum explained variance we will accept. We could have also used the Bayesian or Akaike Information Criteria (BIC or AIC) to determine the optimal number of components; in fact, our implementation of PCA on our data does include a “score” method for the PCA, which gives the average log-likelihood score, from which we can derive the BIC or AIC and set the number of components to be the argument of the minimum for the BIC or AIC.</p>

<p>Once we have orthogonalized the independent variables and transformed the appropriate series for demographic data, we can use an optimization algorithm to minimize the sum of squared residuals of our predictions. Two attractive candidates implemented by SciPy are the Powell Conjugate Direction Method or the Broyden - Fletcher - Goldfarb - Shanno algorithm. The former performs well for continuous but complex functions such as the one we have, due to its non-reliance on differentiation, whereas the latter is a nonlinear optimizer approximating Newton’s Method, which is appropriate since hill climbing algorithms are in general powerful for our situation, where the parameter space does not contain many local optima.</p>

<p>Using one of these optimization algorithms, we can then compute the optimal parameters (i.e. $v_Y, v_K, v_C$). From this, we can back test on our data to examine how strong our model’s fit is.</p>

<p>Given the number of data points we have versus the number of variables we ultimately use after applying principal component analysis, and given the complexity of the functional form we use, overfitting does not immediately seem to be problematic; however, to reinforce the robustness of the model, one area of extension could be using cross-validation instead of regression to determine the optimal parameters.</p>

<p><img src="http://s30.postimg.org/8hcfupbxr/regr_fit.png" alt=""></p>

<h3>
<a id="final-analysis" class="anchor" href="#final-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final Analysis</h3>

<p>In the end, our model fits quite well, which is pleasantly surprising because efforts to model disease outbreak are usually focused on one geographical unit (i.e. region). Our methods have generalized to examine disease outbreak across regions with model parameters dependent on demographic characteristics. Specifically, based on demographic data, we can robustly estimate the size of the population at risk, the speed of infection, and the time from infection to reported cases. In doing so, we have created a tool that not only has predictive power for the future spread of ebola, but also allows us to explore the effects of demographic characteristics on the spread of disease. By studying these effects, in the future we can predict disease hotspots and better understand how socioeconomic changes lead to epidemiological shifts.</p>
      </section>
      <footer>
        <p><small>Hosted on <a href="http://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		
  </body>
</html>
